{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "The purpose of this notebook is to begin modeling the train data from our previous notebook (Step1_DataCleaning_Exploration).\n",
    "\n",
    "Objectives:\n",
    "\n",
    "- Run model with my choice of perameters and use AUC to evaluate model performance\n",
    "- Tune parameters of model to find the best fitting for data\n",
    "- Repeat objective 1 until the optimal score is achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Library for Data Visullizations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "# Datatime Handling\n",
    "import datetime\n",
    "\n",
    "# Testing & Metrics \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Helper functions\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import gc\n",
    "\n",
    "# Model\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boimoriba/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../CSV_Files/cleaned_train', index_col=0) # importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>83230</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:32:21</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17357</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:33:34</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>35810</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:34:12</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>45745</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>478</td>\n",
       "      <td>2017-11-06 14:34:52</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>161007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:35:08</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel           click_time  is_attributed  hour  \\\n",
       "0   83230    3       1  13      379  2017-11-06 14:32:21              0    14   \n",
       "1   17357    3       1  19      379  2017-11-06 14:33:34              0    14   \n",
       "2   35810    3       1  13      379  2017-11-06 14:34:12              0    14   \n",
       "3   45745   14       1  13      478  2017-11-06 14:34:52              0    14   \n",
       "4  161007    3       1  13      379  2017-11-06 14:35:08              0    14   \n",
       "\n",
       "   day  \n",
       "0    6  \n",
       "1    6  \n",
       "2    6  \n",
       "3    6  \n",
       "4    6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # quick view of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000000 entries, 0 to 9999999\n",
      "Data columns (total 9 columns):\n",
      "ip               int64\n",
      "app              int64\n",
      "device           int64\n",
      "os               int64\n",
      "channel          int64\n",
      "click_time       object\n",
      "is_attributed    int64\n",
      "hour             int64\n",
      "day              int64\n",
      "dtypes: int64(8), object(1)\n",
      "memory usage: 762.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info() # click_time did not remain as datetime so it must be changed before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # quick view of how large the data is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Alterations\n",
    "Before we begin the modeling process as you can see the click_time column is still a string vs a datetimes64 [ns]. This is just one small but important example as to why you need to look at your data even after you have cleaned and transformed it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['click_time'] = pd.to_datetime(df['click_time']) # set click_time as timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000000 entries, 0 to 9999999\n",
      "Data columns (total 9 columns):\n",
      "ip               int64\n",
      "app              int64\n",
      "device           int64\n",
      "os               int64\n",
      "channel          int64\n",
      "click_time       datetime64[ns]\n",
      "is_attributed    int64\n",
      "hour             int64\n",
      "day              int64\n",
      "dtypes: datetime64[ns](1), int64(8)\n",
      "memory usage: 762.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info() # checking my work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting my target and and df to run test on and with\n",
    "X= df.drop(columns=['is_attributed', 'click_time'])\n",
    "y= df['is_attributed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: the parameter stratify is being used becasue we have an imbalanced dataset. It deals with this type of data by spliting the data in a way to where the proportion of the sampled values matches the proportion of the values given to the stratify parameter, in our case our target. Check resource section of notebook for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model: Model 1 \n",
    "Remember I just want a baseline to see where I'm starting, so the parameters will be set based off of the type of problem we are dealing with; binary classification. Se recource section for more information on model and parameter selection. Lets fit the model onto the train data and see how well it does using the AUC metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training data into LightGBM dataset format\n",
    "# mandatory for using lgb, see blog link in resource section \n",
    "df_train = lgb.Dataset(X_train, label=y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Dataset at 0x1a1b48c400>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train # what the dataset has turned into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameters I choice for this dataset, see blog link in resource section \n",
    "params = {}\n",
    "params['learning_rate'] = 0.003\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['application'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 50\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 10\n",
    "params['bagging_fraction'] = 0.5\n",
    "clf = lgb.train(params, df_train) # training model on parameters and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(X_test) # assigning model predictions to variable to be called later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.9446001248781845\n"
     ]
    }
   ],
   "source": [
    "roc_auc = roc_auc_score(y_test,y_pred) # checking how well the model scored \n",
    "print(\"ROC AUC Score: {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Observation\n",
    "From what the results show this model is about 94% confident in predicting if an app is downloaded or not. What I would like to do next is conduct a grid search to find the best parameters for this model and classification problem. Being that this is such a large dataset, that will take up time I do not have for this project. So what I will have to do is down sample the data to a smaller portion that will take a shorter period of time to run a grid search. Please see resource section of notebook for further information on the medeling of this data (winner of competition strategy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down Sampling \n",
    "Below this markdown cell you will see the imbalance in the data when it comes to app downloads vs non downloads. We want to make them value count of each match up. To down sample I will make the values that are dominant in the column equal to the values that are least dominant in this column. After speaking with our DS 02172020 Lead Instructor Bryan Arnold, a function shown to me that can downsample the data for me vs writing a custom function!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9981283\n",
       "1      18717\n",
       "Name: is_attributed, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts() # view of the imbalance in the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 18717, 1: 18717})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boimoriba/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# downsampling and printing out the results \n",
    "rus = RandomUnderSampler(random_state=42) \n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split On Balanced Data\n",
    "This must be done but to the new X and y from the above cell. X_res, y_res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37434, 7)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res.shape # just looking at the data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37434,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res.shape # just looking at the data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.25, random_state = 0)\n",
    "# taking out stratify parameter because data is now balanced "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch: Model 4320\n",
    "Now that I have a smaller dataset to deal with after downsampling, I can now perform a grid search to find the best parameters for modeling this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''See resource section to find a blog post with this code. \n",
    "This function will take in the train test split data, lbg model, parameters to filter through for lbg,\n",
    "cross validate the data 10 times, score the data using AUC metric, \n",
    "without performing any probabilities as its arguments\n",
    "\n",
    "What it will return when given the parameters it needs, will be the AUC score of the best fitting parameters for \n",
    "the dataset\n",
    "'''\n",
    "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=10, scoring_fit='roc_auc',\n",
    "                       do_probabilities = False):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring_fit,\n",
    "        verbose=2\n",
    "    )\n",
    "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
    "    \n",
    "    if do_probabilities:\n",
    "      pred = fitted_model.predict_proba(X_test_data)\n",
    "    else:\n",
    "      pred = fitted_model.predict(X_test_data)\n",
    "    \n",
    "    return fitted_model, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   57.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 704 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1266 tasks      | elapsed: 23.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1762 tasks      | elapsed: 34.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed: 34.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9537882530559942\n",
      "{'application': 'binary', 'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'learning_rate': 0.003, 'max_depth': 15, 'metric': 'auc', 'min_data': 50, 'n_estimators': 1000, 'num_leaves': 50}\n"
     ]
    }
   ],
   "source": [
    "'''Below I am creating a variable to hold the model for the function in the previous cell. I am also creating a \n",
    "dictionary of parameters that will be used in the gridsearch. Once the model and parameters are set I am then \n",
    "calling the function with the needed arguments. Once the function is done running I will recieve an AUC score \n",
    "and the list of parameters that got me that score.\n",
    "See resource section for more info on the code below.'''\n",
    "\n",
    "model = lgb.LGBMClassifier() # model I am using\n",
    "\n",
    "# parameters I chose to try for gridsearch\n",
    "param_grid = {\n",
    "'n_estimators': [400, 700, 1000],\n",
    "'learning_rate': [0.003], \n",
    "'boosting_type': ['gbdt', 'rf'],\n",
    "'application':['binary'], \n",
    "'metric': ['auc', 'binary_logloss'],\n",
    "'num_leaves': [50, 100, 200],\n",
    "'min_data': [50,100],\n",
    "'max_depth': [15, 20, 25],\n",
    "'bagging_fraction': [0.5, 1, 5, 7]\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n",
    "                                 param_grid, cv=5, scoring_fit='roc_auc') # calling the function\n",
    "\n",
    "# printing resuts of the gridsearch and the score\n",
    "print(model.best_score_)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Observation\n",
    "ROC AUC Score: 0.9446001248781845 was our score before balancing the data and a spesific list of parameters. After running through 4320 different models, now the model is scoring ROC AUC Score: 0.9537882530559942, due to a gridsearch that found the best fitting parameters. Before I move on, I want to try another set of parameters to see how the model does. Below you will see a new dictionary of parameters, see resource section for more info on the code below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying a new set of parameters \n",
    "param_grid2 = {\n",
    "'learning_rate': [0.10],\n",
    "'num_leaves': [7],  \n",
    "'max_depth': [3],  \n",
    "'min_child_samples': [100], \n",
    "'max_bin': [100],  \n",
    "'subsample': [0.7], \n",
    "'subsample_freq': [1],  \n",
    "'colsample_bytree': [0.9],  \n",
    "'min_child_weight': [0],  \n",
    "'scale_pos_weight':[200]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting new (balanced data) training data into LightGBM dataset format\n",
    "# mandatory for using lgb, see blog link in resource section \n",
    "df_train2 = lgb.Dataset(X_train, label=y_train, feature_name=['ip', 'app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "                       ) \n",
    "\n",
    "\n",
    "clf = lgb.train(params, df_train2) # training model on new parameters and balanced train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(X_test) # assigning model predictions to variable to be called later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.9554002673539395\n"
     ]
    }
   ],
   "source": [
    "roc_auc = roc_auc_score(y_test,y_pred) # checking how well the model scored \n",
    "print(\"ROC AUC Score: {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "When determining fraudulent and non fraudulent clicks, what are the most important features/ columns in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_importance(clf, max_num_features=10, grid=False, color='black') # plotting feature importance for lgb\n",
    "plt.savefig('fi.png', transparent= True) # saving plot as image without white background\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "The last version of my model did great! 0.955 Score For AUC is as far as I could get with the time frame that we had on this project. Future recommendations I have would be to gather more data, more data is always good. This will help me teach the model more about patterns in the data. I would also follow in the foot steps of the TalkData competition winners & pull more features from the click time column, in this project I only had two features I pulled from the click time column, but with more time and research I could have pulled more, which would have helped my model learn more patterns to help identify fraudulent behavior in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "**Code for importing lgb:** https://www.kaggle.com/pranav84/lightgbm-fixing-unbalanced-data-lb-0-9680\n",
    "\n",
    "\n",
    "**Parameter Tuning for lgb:** https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "\n",
    "\n",
    "**Blog and code on lgb:** https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "\n",
    "\n",
    "**Strategy of TalkingData AdTracking Fraud Detection Challenge Winner:** https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56475?rvi=1\n",
    "\n",
    "\n",
    "**Explanation of train tes split parameter stratify:** https://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn (The second answer has a short explanation)\n",
    "\n",
    "\n",
    "**Down Sampling:** https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html#imblearn.under_sampling.RandomUnderSampler \n",
    "\n",
    "\n",
    "**GridSearch function on lgb:** https://mlfromscratch.com/gridsearch-keras-sklearn/#/\n",
    "\n",
    "\n",
    "**Parameter Grid 2:** https://www.kaggle.com/asraful70/notebook-version-of-talkingdata-lb-0-9786\n",
    "\n",
    "\n",
    "**Plotting Feature Importance for lgb:** https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html#plot_importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
